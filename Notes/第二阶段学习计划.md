# Kubernetes 学习路径 - 第二阶段（筑基期）详细学习计划

## 🎯 阶段目标
- **时间安排**: 4-6 周，每周 8-10 小时（总计 32-60 小时）
- **核心目标**: 
  - 深入理解 Kubernetes 基本架构
  - 掌握 Kubernetes 容器调度的基本流程

## 🗺️ 整体学习路径图

```
第1周：基础架构认知
架构概览 → API Server → Etcd 存储
   ⬇️
第2周：控制和调度机制  
控制器模式 → 调度算法 → 整体流程
   ⬇️
第3周：节点组件深入
Kubelet → 容器运行时 → 网络组件
   ⬇️
第4周：核心插件实战
DNS 服务 → 网络插件 → 手动部署
   ⬇️
第5-6周：高级综合实战
交互分析 → 故障排查 → 性能优化
```

## 📋 学习曲线设计原则

- **🎯 渐进式难度**：从概念理解 → 机制分析 → 实践操作 → 故障排查
- **🔗 依赖关系清晰**：每个阶段都基于前一阶段的知识
- **⚖️ 理论实践平衡**：每个概念都有对应的动手实验
- **🎪 灵活性**：标记了优先级，可根据时间调整学习重点

## 🏷️ 优先级标签说明

- **🔥 基础必修/核心必修**：必须掌握，是后续学习的基础
- **⭐ 综合提升/综合应用**：重要的综合性内容，强烈建议完成
- **🟡 进阶选修/深入理解**：深入内容，时间充裕时学习
- **💡 学习建议**：针对该模块的具体学习策略
- **⚠️ 注意事项**：学习过程中需要注意的问题

## 📅 详细周计划

### 第1周：理论基础和架构概览（8-10小时）
**学习主线**：从宏观到微观，先整体架构 → API Server → 数据存储

#### 📅 Day 1-2: Control Plane & Node 架构理解（3-4小时）
**学习优先级**：🔥 基础必修 | **前置要求**：完成环境安装
**学习内容**：
- [ ] 阅读官方文档：[Kubernetes Components](https://kubernetes.io/docs/concepts/overview/components/)
- [ ] 理解 Control Plane（原 Master）和 Node 的关系
- [ ] 学习组件间的通信机制

**详细实践步骤**：

**步骤1：环境准备和集群信息获取**
**🎯 学习目的**：建立对 Kubernetes 集群整体结构的直观认识，了解集群的基本组成
**🔗 整体关系**：这是整个架构学习的起点，为后续深入各个组件打下基础

```bash
# 1.1 启动本地集群
minikube start
```

**💻 命令输出记录：**
```
[在此记录 minikube start 的完整输出]


```

```bash
# 1.2 验证集群状态
kubectl cluster-info
```

**💻 命令输出记录：**
```
[在此记录 kubectl cluster-info 的输出，注意 control plane 地址]


```

```bash
# 1.3 查看节点详细信息
kubectl get nodes -o wide
```

**💻 命令输出记录：**
```
[记录节点信息：名称、状态、角色、版本、内部IP、外部IP、操作系统、内核、容器运行时]


```

```bash
kubectl describe nodes
```

**💻 命令输出记录（关键信息）：**
```
[记录重要信息：
- 节点容量（CPU、内存、存储）
- 节点条件（Ready、MemoryPressure、DiskPressure等）
- 已分配资源情况
- 系统信息]


```

**📝 本步骤学习收获总结：**
```
[记录你对集群整体架构的理解]


```

**步骤2：系统组件探索**
**🎯 学习目的**：识别和理解 Kubernetes 核心组件，建立组件与功能的对应关系
**🔗 整体关系**：这是理解 Kubernetes 架构的核心步骤，为后续深入每个组件做准备

```bash
# 2.1 查看所有系统组件 Pod
kubectl get pods -n kube-system -o wide
```

**💻 命令输出记录：**
```
[记录所有系统组件Pod的信息：
- kube-apiserver-*（API服务器）
- etcd-*（数据存储）
- kube-scheduler-*（调度器）
- kube-controller-manager-*（控制器管理器）
- kube-proxy-*（网络代理）
- coredns-*（DNS服务）
- storage-provisioner（存储供应商）]


```

```bash
# 2.2 查看API Server组件详细信息
kubectl describe pod -n kube-system <kube-apiserver-pod>
```

**💻 命令输出记录（关键信息）：**
```
[记录API Server的关键配置：
- 启动参数（如--secure-port、--etcd-servers等）
- 资源使用情况
- 挂载的卷信息]


```

```bash
# 2.3 查看Etcd组件详细信息
kubectl describe pod -n kube-system <etcd-pod>
```

**💻 命令输出记录（关键信息）：**
```
[记录Etcd的关键配置：
- 数据目录配置
- 集群配置
- 证书配置]


```

```bash
# 2.4 查看组件运行状态和资源使用
kubectl top pod -n kube-system
```

**💻 命令输出记录：**
```
[记录各组件的CPU和内存使用情况，了解组件的资源消耗]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. 你识别出了哪些核心组件？
2. 每个组件的主要配置参数是什么？
3. 组件间的资源消耗对比如何？]


```

**步骤3：组件配置和通信分析**
**🎯 学习目的**：深入了解组件配置细节和组件间的通信机制
**🔗 整体关系**：这是从表面现象深入到内部机制的关键步骤，为理解 Kubernetes 工作原理建立基础

```bash
# 3.1 查看 API Server 的配置参数
kubectl get pod -n kube-system <kube-apiserver-pod> -o yaml
```

**💻 命令输出记录（关键配置）：**
```
[记录API Server的重要配置参数：
- --etcd-servers：etcd连接地址
- --secure-port：安全端口
- --service-cluster-ip-range：Service IP范围
- --admission-control：准入控制器列表]


```

```bash
# 3.2 检查组件间的网络通信 - Services
kubectl get svc -n kube-system
```

**💻 命令输出记录：**
```
[记录系统服务：
- kubernetes：API Server服务
- kube-dns/coredns：DNS服务
- 各服务的ClusterIP和端口信息]


```

```bash
# 3.3 检查组件间的网络通信 - Endpoints
kubectl get endpoints -n kube-system
```

**💻 命令输出记录：**
```
[记录服务端点信息，了解服务如何映射到具体的Pod]


```

```bash
# 3.4 查看集群网络配置
kubectl get configmap -n kube-system
```

**💻 命令输出记录：**
```
[记录重要的ConfigMap，特别注意：
- kubeadm-config：集群配置
- kube-proxy：网络代理配置
- coredns：DNS配置]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. API Server是如何配置etcd连接的？
2. 组件间通过什么方式进行通信？
3. 集群网络是如何配置的？]


```

**步骤4：架构图绘制与理解**
**🎯 学习目的**：将观察到的信息系统化，形成完整的架构认知
**🔗 整体关系**：这是第1周学习的总结和升华，为后续深入学习建立知识框架

**📋 实践任务清单：**
- [ ] 创建文档记录观察到的组件信息
- [ ] 使用工具（如 draw.io、Lucidchart）或手绘架构图  
- [ ] 标注组件间的通信关系和端口

**📝 架构图绘制记录：**
```
[在此粘贴或描述你绘制的架构图，包括：
1. Control Plane组件及其关系
2. Node组件及其关系
3. 组件间的通信路径和端口
4. 数据流向]


```

**知识点检查**：
- 能说出 Control Plane 包含哪些组件？
- Node 节点包含哪些核心组件？
- 各组件之间如何通信？
- 每个组件的主要配置参数是什么？

#### 📅 Day 3-4: API Server 深入理解（3-4小时）
**学习优先级**：🔥 基础必修 | **前置要求**：完成 Day 1-2 架构理解
**依赖关系**：需要先理解整体架构，再深入 API Server 细节
**学习内容**：
- [ ] API Server 的职责和工作原理
- [ ] RESTful API 设计理念
- [ ] 请求处理流程：认证、授权、准入控制

**详细实践步骤**：

**步骤1：API Server 代理访问**
**🎯 学习目的**：理解 kubectl proxy 的工作原理，学习直接与 API Server 交互的方法
**🔗 整体关系**：基于第1周的架构理解，深入学习 API Server 这个最核心的组件

```bash
# 1.1 启动 kubectl proxy（在新终端窗口运行）
kubectl proxy --port=8080
```

**💻 命令输出记录：**
```
[记录proxy启动信息，注意代理监听的地址和端口]


```

```bash
# 1.2 通过代理访问 API Server - 查看API版本
curl http://localhost:8080/api/v1
```

**💻 命令输出记录：**
```
[记录API v1版本的资源列表，注意有哪些核心资源类型]


```

```bash
# 1.3 查看所有命名空间
curl http://localhost:8080/api/v1/namespaces
```

**💻 命令输出记录：**
```
[记录所有命名空间信息，理解命名空间的JSON结构]


```

```bash
# 1.4 查看节点信息
curl http://localhost:8080/api/v1/nodes
```

**💻 命令输出记录：**
```
[记录节点的完整JSON结构，对比kubectl get nodes的输出差异]


```

```bash
# 1.5 探索所有API组
curl http://localhost:8080/apis
```

**💻 命令输出记录：**
```
[记录所有API组信息：apps、extensions、batch等，了解API的分组结构]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. kubectl proxy是如何工作的？
2. API Server的RESTful接口结构是怎样的？
3. 你发现了哪些不同的API组和版本？]


```

**步骤2：直接 API 调用实验**
**🎯 学习目的**：学习在没有 kubectl proxy 的情况下直接访问 API Server，理解认证机制
**🔗 整体关系**：从代理访问过渡到直接访问，深入理解 Kubernetes 的认证授权机制

```bash
# 2.1 获取集群 API Server 地址
kubectl cluster-info
```

**💻 命令输出记录：**
```
[记录API Server的完整地址，注意HTTPS端口和证书信息]


```

```bash
# 2.2 获取访问令牌
TOKEN=$(kubectl create token default)
echo "Token: $TOKEN"
```

**💻 命令输出记录：**
```
[记录生成的JWT令牌（注意：实际使用时要保护好令牌安全）]


```

```bash
# 2.3 获取API Server地址变量
APISERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')
echo "API Server: $APISERVER"
```

**💻 命令输出记录：**
```
[记录从kubeconfig中提取的API Server地址]


```

```bash
# 2.4 使用令牌直接访问 API Server
curl -X GET $APISERVER/api/v1/namespaces/default/pods \
  --header "Authorization: Bearer $TOKEN" \
  --insecure
```

**💻 命令输出记录：**
```
[记录API响应结果，注意JSON结构和认证是否成功]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. 直接访问API Server需要哪些信息？
2. JWT令牌是如何工作的？
3. 相比kubectl proxy，直接访问有什么优缺点？]


```

**步骤3：API 操作实验**
**🎯 学习目的**：通过创建和管理资源来理解 API Server 的 CRUD 操作
**🔗 整体关系**：从只读操作转向写操作，理解 API Server 如何处理资源的完整生命周期

```bash
# 3.1 创建一个简单的 Pod YAML 文件
cat > test-pod.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
EOF
```

**💻 命令输出记录：**
```
[记录YAML文件创建成功的确认信息]


```

```bash
# 3.2 使用 kubectl apply 创建 Pod
kubectl apply -f test-pod.yaml
```

**💻 命令输出记录：**
```
[记录Pod创建的结果，注意是"created"还是"configured"状态]


```

```bash
# 3.3 通过API直接查询创建的 Pod
curl -X GET $APISERVER/api/v1/namespaces/default/pods/test-pod \
  --header "Authorization: Bearer $TOKEN" \
  --insecure | jq .
```

**💻 命令输出记录：**
```
[记录Pod的完整JSON结构，特别关注：
- metadata（名称、标签、注解）
- spec（期望状态）
- status（当前状态）]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. YAML如何转换为API调用？
2. Pod的JSON结构包含哪些重要字段？
3. spec和status有什么区别？]


```

**步骤4：API Server 日志分析**
**🎯 学习目的**：通过日志分析理解 API Server 的内部工作机制和请求处理流程
**🔗 整体关系**：从外部观察转向内部机制，理解 API Server 如何处理各种请求

```bash
# 4.1 查看 API Server 历史日志
kubectl logs -n kube-system -l component=kube-apiserver --tail=50
```

**💻 命令输出记录：**
```
[记录重要的日志信息：
- 认证相关日志
- 授权决策日志
- 资源操作日志
- 错误或警告信息]


```

```bash
# 4.2 在新终端启动实时日志监控
kubectl logs -n kube-system -l component=kube-apiserver -f
```

**💻 命令输出记录：**
```
[记录：准备实时监控API Server日志]


```

```bash
# 4.3 执行操作并观察日志变化
kubectl get pods
```

**💻 命令输出记录：**
```
[记录kubectl get pods的输出，然后切换到日志终端观察对应的API调用]


```

```bash
kubectl create deployment nginx-test --image=nginx:latest
```

**💻 命令输出记录：**
```
[记录创建deployment的结果，观察日志中的相关记录]


```

```bash
kubectl delete deployment nginx-test
```

**💻 命令输出记录：**
```
[记录删除操作的结果，观察日志中的删除相关记录]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. API Server日志包含哪些关键信息？
2. 不同操作在日志中有什么不同的表现？
3. 你能从日志中看出请求的处理流程吗？]


```

**步骤5：API 版本和资源探索**
**🎯 学习目的**：深入理解 Kubernetes API 的版本控制和资源结构设计
**🔗 整体关系**：完善对 API Server 功能的全面理解，为后续学习其他组件建立 API 基础

```bash
# 5.1 查看所有可用的 API 版本
kubectl api-versions
```

**💻 命令输出记录：**
```
[记录所有API版本，注意：
- 核心API（v1）
- 应用API（apps/v1）
- 批处理API（batch/v1）
- 扩展API等]


```

```bash
# 5.2 查看 Pod 资源的详细结构
kubectl explain pod
```

**💻 命令输出记录：**
```
[记录Pod资源的顶级字段和描述]


```

```bash
# 5.3 查看 Pod spec 的详细结构
kubectl explain pod.spec
```

**💻 命令输出记录：**
```
[记录Pod spec的所有字段，了解Pod的配置选项]


```

```bash
# 5.4 查看 Deployment 资源结构
kubectl explain deployment
```

**💻 命令输出记录：**
```
[记录Deployment的基本结构，对比与Pod的差异]


```

```bash
# 5.5 查看资源的 API 版本信息
kubectl api-resources | grep -E "(NAME|pods|deployments|services)"
```

**💻 命令输出记录：**
```
[记录关键资源的API版本信息，包括：
- 资源名称（短名称和全名）
- API组和版本
- 是否有命名空间范围]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. Kubernetes如何管理API版本演进？
2. 不同资源类型的API结构有什么共同特征？
3. 如何快速了解一个新资源的配置选项？]


```

**步骤6：清理和环境恢复**
**🎯 学习目的**：学习资源清理的最佳实践，保持环境整洁
**🔗 整体关系**：完成API Server学习的收尾工作，为下一阶段学习做准备

```bash
# 6.1 清理测试资源
kubectl delete pod test-pod
```

**💻 命令输出记录：**
```
[记录资源删除的结果]


```

```bash
# 6.2 停止 proxy 进程（在proxy终端按Ctrl+C）
# 然后验证proxy已停止
ps aux | grep "kubectl proxy"
```

**💻 命令输出记录：**
```
[确认proxy进程已停止]


```

```bash
# 6.3 清理临时文件
rm test-pod.yaml
ls test-pod.yaml 2>/dev/null || echo "文件已成功删除"
```

**💻 命令输出记录：**
```
[确认临时文件已清理]


```

**知识点检查**：
- API Server 如何处理一个 kubectl 请求？
- 什么是 API 版本控制？
- RESTful API 的基本操作有哪些？
- 认证、授权、准入控制的流程是什么？

#### 📅 Day 5-7: Etcd 存储机制（2-3小时）
**学习优先级**：🟡 进阶选修 | **前置要求**：完成 API Server 学习
**依赖关系**：理解 API Server 后，学习数据如何存储在 Etcd 中
**学习建议**：⚠️ 如果时间紧张，可以先跳过深入的 etcd 操作，重点理解概念
**学习内容**：
- [ ] 阅读 [Etcd 官方文档](https://etcd.io/docs)
- [ ] 理解分布式一致性算法（Raft）
- [ ] Kubernetes 如何使用 Etcd

**详细实践步骤**：

**步骤1：Etcd 状态探索**
**🎯 学习目的**：了解 etcd 在 Kubernetes 集群中的运行状态和基本配置
**🔗 整体关系**：基于对 API Server 的理解，深入学习 Kubernetes 的数据存储层

```bash
# 1.1 查看 etcd Pod 状态
kubectl get pods -n kube-system -l component=etcd
```

**💻 命令输出记录：**
```
[记录etcd Pod的基本信息：名称、状态、重启次数、运行时间]


```

```bash
# 1.2 获取 etcd Pod 名称并查看详细配置
ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd -o jsonpath='{.items[0].metadata.name}')
echo "Etcd Pod: $ETCD_POD"
kubectl describe pod -n kube-system $ETCD_POD
```

**💻 命令输出记录：**
```
[记录etcd的关键配置信息：
- 启动参数（--data-dir, --listen-client-urls等）
- 挂载的卷（数据目录、证书等）
- 资源限制和请求]


```

```bash
# 1.3 查看 etcd 运行日志
kubectl logs -n kube-system $ETCD_POD --tail=20
```

**💻 命令输出记录：**
```
[记录etcd的重要日志信息：
- 启动信息
- 集群成员信息
- 任何错误或警告]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. etcd在集群中是如何部署的？
2. etcd的主要配置参数有哪些？
3. 从日志中能看出etcd的健康状况吗？]


```

**步骤2：通过 API Server 查看 Etcd 存储的数据**
**🎯 学习目的**：理解 Kubernetes 资源在 etcd 中的存储结构和数据组织方式
**🔗 整体关系**：连接 API Server 和 etcd，理解数据如何从 API 调用转化为存储

```bash
# 2.1 查看所有 namespace 的存储结构
kubectl get --raw /api/v1/namespaces | jq .
```

**💻 命令输出记录：**
```
[记录namespace的JSON存储结构，注意：
- 每个namespace的完整metadata
- spec和status字段
- 资源的版本信息]


```

```bash
# 2.2 查看默认命名空间中的所有 Pod
kubectl get --raw /api/v1/namespaces/default/pods | jq .
```

**💻 命令输出记录：**
```
[记录Pod在etcd中的存储结构，对比之前API调用的结果]


```

```bash
# 2.3 查看系统配置映射
kubectl get --raw /api/v1/namespaces/kube-system/configmaps | jq '.items[] | select(.metadata.name | contains("kubeadm"))'
```

**💻 命令输出记录：**
```
[记录重要的ConfigMap数据，理解集群配置如何存储]


```

```bash
# 2.4 查看特定资源的存储结构
kubectl get --raw /api/v1/namespaces/default | jq .
```

**💻 命令输出记录：**
```
[记录单个namespace的完整存储结构]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. Kubernetes资源在etcd中是如何组织的？
2. API Server返回的数据与etcd存储的数据有什么关系？
3. 资源的版本信息是如何管理的？]


```

**步骤3：数据变化观察实验**
**🎯 学习目的**：通过实际操作观察数据在 etcd 中的变化过程，理解数据一致性
**🔗 整体关系**：验证对 etcd 存储机制的理解，观察数据的完整生命周期

```bash
# 3.1 创建测试命名空间并观察存储
kubectl create namespace test-etcd
```

**💻 命令输出记录：**
```
[记录namespace创建的结果]


```

```bash
kubectl get --raw /api/v1/namespaces/test-etcd | jq .
```

**💻 命令输出记录：**
```
[记录新创建namespace的完整存储结构]


```

```bash
# 3.2 在命名空间中创建ConfigMap并观察
kubectl create configmap test-config --from-literal=key1=value1 -n test-etcd
```

**💻 命令输出记录：**
```
[记录ConfigMap创建结果]


```

```bash
kubectl get --raw /api/v1/namespaces/test-etcd/configmaps/test-config | jq .
```

**💻 命令输出记录：**
```
[记录ConfigMap的存储结构，注意data字段和metadata]


```

```bash
# 3.3 修改ConfigMap并观察版本变化
kubectl patch configmap test-config -n test-etcd -p '{"data":{"key2":"value2"}}'
```

**💻 命令输出记录：**
```
[记录修改操作的结果]


```

```bash
kubectl get --raw /api/v1/namespaces/test-etcd/configmaps/test-config | jq .
```

**💻 命令输出记录：**
```
[记录修改后的存储结构，对比resourceVersion的变化]


```

```bash
# 3.4 清理测试资源
kubectl delete namespace test-etcd
```

**💻 命令输出记录：**
```
[记录删除操作的结果]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. 资源的创建、修改、删除如何反映在etcd中？
2. resourceVersion是如何变化的？
3. 删除操作是立即生效还是有延迟？]


```

**步骤4：备份恢复机制理解**
**🎯 学习目的**：理解 etcd 的备份恢复机制，了解数据持久化的重要性
**🔗 整体关系**：完善对 etcd 运维知识的理解，为集群管理打下基础

```bash
# 4.1 查看 etcd 的数据目录配置
kubectl get pod -n kube-system $ETCD_POD -o yaml | grep -A 5 -B 5 "data-dir"
```

**💻 命令输出记录：**
```
[记录etcd数据目录的配置路径]


```

```bash
# 4.2 查看数据目录的挂载信息
kubectl get pod -n kube-system $ETCD_POD -o yaml | grep -A 10 -B 5 volumes
```

**💻 命令输出记录：**
```
[记录卷挂载信息，了解数据持久化方式]


```

```bash
# 4.3 了解备份相关的理论知识
echo "=== ETCD 备份命令示例 ==="
echo "etcdctl snapshot save backup.db"
echo "etcdctl snapshot status backup.db"
echo "etcdctl snapshot restore backup.db"
echo "=== 注意：不要在学习环境执行备份恢复操作 ==="
```

**💻 命令输出记录：**
```
[记录备份命令的理论知识]


```

```bash
# 4.4 检查 etcd 集群健康状态（如果可以访问）
kubectl exec -n kube-system $ETCD_POD -- etcdctl endpoint health --endpoints=127.0.0.1:2379 --insecure-skip-tls-verify 2>/dev/null || echo "无法直接访问etcd，这在托管集群中是正常的"
```

**💻 命令输出记录：**
```
[记录健康检查结果或无法访问的说明]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. etcd的数据是如何持久化的？
2. 备份恢复对Kubernetes集群有什么重要意义？
3. 在生产环境中应该如何管理etcd的备份？]


```

**知识点检查**：
- Etcd 解决了什么问题？
- Kubernetes 在 Etcd 中存储了什么数据？
- 数据在 etcd 中是如何组织的？
- 为什么 etcd 使用 Raft 算法？

---

### 第2周：控制器和调度器深入（8-10小时）
**学习主线**：从控制理念到调度实现，先控制器 → 调度器 → 整体流程

#### 📅 Day 1-3: Controller Manager 核心理念（4-5小时）
**学习优先级**：🔥 核心必修 | **前置要求**：完成第1周 API Server 学习
**依赖关系**：需要先理解 API Server，才能理解控制器如何与之交互
**学习建议**：✅ 这是理解 Kubernetes 自动化的关键，务必掌握
**学习内容**：
- [ ] 控制器模式（Controller Pattern）
- [ ] List-Watch 机制原理
- [ ] 常见控制器类型：Deployment、ReplicaSet、Service 等

**详细实践步骤**：

**步骤1：控制器运行状态分析**
**🎯 学习目的**：了解 Controller Manager 的运行状态和配置，为深入理解控制器工作原理打基础
**🔗 整体关系**：基于第1周 API Server 理解，开始学习与 API Server 协作的控制器组件

```bash
# 1.1 查看 Controller Manager 状态
kubectl get pods -n kube-system -l component=kube-controller-manager
```

**💻 命令输出记录：**
```
[记录Controller Manager Pod的状态：
- Pod名称和运行状态
- 重启次数和年龄
- 所在节点信息]


```

```bash
# 1.2 查看 Controller Manager 日志
kubectl logs -n kube-system -l component=kube-controller-manager --tail=50
```

**💻 命令输出记录：**
```
[记录关键日志信息：
- 启动的控制器列表
- 任何错误或警告信息
- 控制器的运行状态更新]


```

```bash
# 1.3 查看控制器配置
kubectl describe pod -n kube-system <controller-manager-pod>
```

**💻 命令输出记录：**
```
[记录重要配置参数：
- 启用的控制器列表（--controllers参数）
- 工作节点同步周期
- 资源配额和限制设置
- 挂载的配置文件]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. Controller Manager包含哪些控制器？
2. 控制器是如何启动和配置的？
3. 从日志中能看出控制器的哪些行为？]


```

**步骤2：Deployment 控制器实验**
**🎯 学习目的**：通过实际操作理解 Deployment 控制器如何创建和管理 Pod，观察控制器模式的工作过程
**🔗 整体关系**：从理论转向实践，验证控制器如何通过 API Server 实现资源管理

```bash
# 2.1 创建 Deployment 并实时观察
kubectl create deployment nginx-test --image=nginx:latest --replicas=3
```

**💻 命令输出记录：**
```
[记录Deployment创建结果和初始状态]


```

```bash
# 2.2 在另一个终端实时监控事件（保持运行）
kubectl get events --watch &
```

**💻 命令输出记录（持续更新）：**
```
[记录实时事件流，特别关注：
- Deployment 创建事件
- ReplicaSet 创建事件  
- Pod 调度和创建事件
- 各事件的时间顺序]


```

```bash
# 2.3 观察资源创建过程
kubectl get deployments,replicasets,pods -l app=nginx-test --watch
```

**💻 命令输出记录：**
```
[记录资源状态变化过程：
- Deployment 状态：READY, UP-TO-DATE, AVAILABLE
- ReplicaSet 状态和期望副本数
- Pod 状态从 Pending -> Running 的转变]


```

```bash
# 2.4 查看资源的 owner references - ReplicaSet
kubectl get replicaset -l app=nginx-test -o yaml | grep -A 5 ownerReferences
```

**💻 命令输出记录：**
```
[记录ReplicaSet的所有者信息，应该显示Deployment为owner]


```

```bash
# 2.5 查看资源的 owner references - Pod
kubectl get pods -l app=nginx-test -o yaml | grep -A 5 ownerReferences
```

**💻 命令输出记录：**
```
[记录Pod的所有者信息，应该显示ReplicaSet为owner]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. Deployment -> ReplicaSet -> Pod 的创建顺序是什么？
2. OwnerReferences 如何建立资源间的父子关系？
3. 从事件中能看出控制器的哪些决策过程？]


```

**步骤3：控制器自愈能力测试**
**🎯 学习目的**：验证 Kubernetes 控制器的自愈和自动恢复能力，理解声明式 API 的核心价值
**🔗 整体关系**：通过破坏性测试加深对控制器模式的理解，为理解生产环境的可靠性做准备

```bash
# 3.1 删除单个 Pod，观察恢复过程
POD_NAME=$(kubectl get pods -l app=nginx-test -o jsonpath='{.items[0].metadata.name}')
echo "将要删除的Pod: $POD_NAME"
kubectl delete pod $POD_NAME
```

**💻 命令输出记录：**
```
[记录删除操作的结果和Pod名称]


```

```bash
# 3.2 观察新 Pod 创建过程（在另一个终端窗口运行）
kubectl get pods -l app=nginx-test --watch
```

**💻 命令输出记录：**
```
[记录Pod自动恢复过程：
- 被删除Pod的状态变化
- 新Pod的创建时间
- 新Pod的调度和启动过程
- 整个恢复耗时]


```

```bash
# 3.3 查看相关事件
kubectl get events --field-selector reason=Created,reason=Started --sort-by='.lastTimestamp'
```

**💻 命令输出记录：**
```
[记录与Pod重建相关的事件：
- ReplicaSet的决策事件
- Pod创建事件
- Pod调度事件
- 容器启动事件]


```

```bash
# 3.4 删除 ReplicaSet，观察恢复
RS_NAME=$(kubectl get rs -l app=nginx-test -o jsonpath='{.items[0].metadata.name}')
echo "将要删除的ReplicaSet: $RS_NAME"
kubectl delete rs $RS_NAME
```

**💻 命令输出记录：**
```
[记录ReplicaSet删除结果]


```

```bash
# 3.5 观寺ReplicaSet和Pod的恢复过程
kubectl get rs,pods -l app=nginx-test --watch
```

**💻 命令输出记录：**
```
[记录Deployment控制器如何自动重建：
- 新ReplicaSet的创建
- 新Pod的批量创建过程
- 整个恢复的时间表
- 控制器的决策逻辑]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. 单个Pod失败后，哪个控制器负责恢复？
2. ReplicaSet被删除后，Deployment控制器如何应对？
3. 控制器的自愈能力是如何实现的？
4. 这种自动恢复机制在生产中的意义是什么？]


```

**步骤4：扩缩容控制器行为观察**
**🎯 学习目的**：理解 Deployment 控制器如何处理扩缩容操作，观察控制器对期望状态变化的响应
**🔗 整体关系**：从静态管理转向动态调整，理解控制器如何响应工作负载变化

```bash
# 4.1 扩容操作 - 从3个副本扩展到5个
kubectl scale deployment nginx-test --replicas=5
```

**💻 命令输出记录：**
```
[记录扩容命令的执行结果]


```

```bash
# 4.2 实时观察扩容过程
kubectl get pods -l app=nginx-test --watch
```

**💻 命令输出记录：**
```
[记录Pod数量从3变到5的过程：
- 新Pod的创建时间点
- Pod状态从 Pending 到 Running
- 所有Pod的最终状态]


```

```bash
# 4.3 缩容操作 - 从5个副本减少到2个
kubectl scale deployment nginx-test --replicas=2
```

**💻 命令输出记录：**
```
[记录缩容命令的执行结果]


```

```bash
# 4.4 实时观察缩容过程
kubectl get pods -l app=nginx-test --watch
```

**💻 命令输出记录：**
```
[记录Pod数量从5减少到2的过程：
- 哪些Pod被终止（注意终止策略）
- Pod状态从 Running 到 Terminating
- 终止过程的时间成本]


```

```bash
# 4.5 查看扩缩容事件和历史
kubectl describe deployment nginx-test
```

**💻 命令输出记录：**
```
[记录Deployment的详细信息：
- 当前的replicas设置
- ReplicaSet的更新历史
- 扩缩容相关事件
- Conditions状态变化]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. 扩容和缩容操作的响应速度如何？
2. 缩容时Kubernetes如何选择要终止的Pod？
3. 扩缩容操作在生产中的应用场景是什么？
4. ReplicaSet在扩缩容过程中的作用是什么？]


```

**清理操作**：
```bash
kubectl delete deployment nginx-test
```

**知识点检查**：
- 什么是声明式 API？
- List-Watch 如何实现实时同步？
- 控制循环的工作原理？
- OwnerReferences 的作用是什么？

#### 📅 Day 4-5: Scheduler 调度机制（3-4小时）
**学习优先级**：🔥 核心必修 | **前置要求**：完成 Controller Manager 学习
**依赖关系**：理解控制器创建 Pod 后，学习调度器如何分配 Pod 到节点
**学习建议**：✅ 与控制器配合，形成完整的 Pod 创建流程理解
**学习内容**：
- [ ] 阅读：[Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/)
- [ ] 调度算法：预选（Predicate）和优选（Priority）
- [ ] 资源请求和限制对调度的影响

**详细实践步骤**：

**步骤1：调度器状态分析**
**🎯 学习目的**：了解 Scheduler 的运行状态和配置，为深入理解调度算法打基础
**🔗 整体关系**：在理解控制器创建 Pod 后，学习调度器如何将 Pod 分配到合适的节点

```bash
# 1.1 查看调度器 Pod 状态
kubectl get pods -n kube-system -l component=kube-scheduler
```

**💻 命令输出记录：**
```
[记录Scheduler Pod的状态信息：
- Pod名称和运行状态
- 重启次数和年龄
- 所在节点位置]


```

```bash
# 1.2 查看调度器日志（重点关注调度决策）
kubectl logs -n kube-system -l component=kube-scheduler --tail=50
```

**💻 命令输出记录：**
```
[记录调度器日志中的关键信息：
- 调度成功的Pod信息
- 调度失败的原因
- 算法执行时间统计
- 任何错误或警告信息]


```

```bash
# 1.3 查看调度器配置
kubectl describe pod -n kube-system <scheduler-pod-name>
```

**💻 命令输出记录：**
```
[记录调度器的重要配置参数：
- 调度算法配置（--policy-config-file）
- 绑定超时设置
- 节点亲和性配置
- 资源限制设置]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. Scheduler的运行状态是否正常？
2. 从日志中能看出调度器的哪些工作模式？
3. 调度器的配置参数中哪些对性能影响最大？]


```

**步骤2：基本调度实验**
**🎯 学习目的**：通过创建简单 Pod 观察调度器的工作流程，理解基本调度决策过程
**🔗 整体关系**：从调度器状态转向实际调度操作，观察理论在实践中的表现

```bash
# 2.1 创建简单的 Pod 观察调度过程
kubectl run simple-pod --image=nginx:latest
```

**💻 命令输出记录：**
```
[记录Pod创建结果]


```

```bash
# 2.2 实时监控调度事件（在另一个终端窗口运行）
kubectl get events --watch | grep -i schedul &
```

**💻 命令输出记录（持续更新）：**
```
[记录调度相关事件：
- 调度成功事件：Successfully assigned
- 调度失败事件：Failed to schedule
- 调度时间和目标节点]


```

```bash
# 2.3 查看 Pod 调度结果 - 详细信息
kubectl get pod simple-pod -o wide
```

**💻 命令输出记录：**
```
[记录Pod的调度结果：
- 被调度到的节点
- Pod IP地址
- 运行状态和就绪状态]


```

```bash
# 2.4 查看 Pod 详细信息和调度事件
kubectl describe pod simple-pod
```

**💻 命令输出记录：**
```
[记录Pod详细信息中的调度相关内容：
- Node字段：指定调度的节点
- Events部分：调度成功事件
- 调度的时间戳和理由]


```

```bash
# 2.5 查看节点资源情况 - 理解调度依据
kubectl describe nodes
```

**💻 命令输出记录：**
```
[记录关键节点信息：
- 节点标签（Labels）和污点（Taints）
- 资源分配情况：CPU和内存使用率
- 节点条件（Conditions）状态
- 当前运行Pod数量]


```

```bash
# 2.6 查看节点实时资源使用情况
kubectl top nodes
```

**💻 命令输出记录：**
```
[记录实时资源使用情况：
- 各节点的CPU和内存使用率
- 资源使用的绝对值和百分比
- 哪些节点的资源相对充裕]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. Pod从创建到被调度的完整流程是怎样的？
2. 调度器如何在多个节点中选择最适合的？
3. 从事件和日志中能看出调度器的决策过程吗？
4. 节点资源情况如何影响调度决策？]


```

**步骤3：资源请求调度实验**
**🎯 学习目的**：理解资源请求和限制对调度决策的影响，学习调度器如何处理资源约束
**🔗 整体关系**：从简单调度转向带约束的调度，理解生产环境中资源管理的重要性

```bash
# 3.1 创建有资源请求的 Pod
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: resource-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    resources:
      requests:
        cpu: "0.5"
        memory: "256Mi"
      limits:
        cpu: "1"
        memory: "512Mi"
EOF
```

**💻 命令输出记录：**
```
[记录带资源限制Pod的创建结果]


```

```bash
# 3.2 观察调度过程 - 查看Pod详情
kubectl describe pod resource-pod
```

**💻 命令输出记录：**
```
[记录调度相关信息：
- Pod的资源requests和limits设置
- 调度到的节点和调度时间
- 调度器是否考虑了资源约束]


```

```bash
# 3.3 查看调度相关事件
kubectl get events --field-selector involvedObject.name=resource-pod
```

**💻 命令输出记录：**
```
[记录resource-pod的事件列表：
- 调度成功/失败事件
- 调度器的决策信息
- 是否有资源不足的警告]


```

```bash
# 3.4 查看节点资源分配情况
kubectl describe nodes | grep -A 5 "Allocated resources"
```

**💻 命令输出记录：**
```
[记录各节点的资源分配：
- 已分配的CPU和内存资源
- 可用资源余量
- 这个Pod的资源占用情况
- 节点资源使用率对比]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. 资源requests和limits的区别和作用是什么？
2. 调度器如何根据资源请求选择节点？
3. 资源约束对调度决策速度有何影响？
4. 在生产中如何合理设置资源参数？]


```

**步骤4：调度约束实验（资源不足场景）**
**🎯 学习目的**：理解调度失败的原因和机制，学习如何诊断和解决调度问题
**🔗 整体关系**：通过故意创建不可调度的Pod，加深对调度约束的理解

```bash
# 4.1 创建不可调度的 Pod（资源过大）
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: large-resource-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    resources:
      requests:
        cpu: "10"
        memory: "50Gi"
EOF
```

**💻 命令输出记录：**
```
[记录大资源Pod的创建结果]


```

```bash
# 4.2 查看调度失败的详细原因
kubectl describe pod large-resource-pod
```

**💻 命令输出记录：**
```
[记录调度失败的详细信息：
- Pod的状态（应该为Pending）
- Events中的失败原因
- 调度器给出的具体错误信息
- 不满足条件的节点数量]


```

```bash
# 4.3 查看调度相关事件
kubectl get events --field-selector involvedObject.name=large-resource-pod
```

**💻 命令输出记录：**
```
[记录调度失败的事件列表：
- FailedScheduling事件的发生时间
- 具体的错误原因：0/X nodes are available
- 不满足资源要求的节点列表
- 调度器的重试情况]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. 调度失败时Pod会处于什么状态？
2. 如何从事件中分析调度失败的具体原因？
3. 调度器如何处理资源不足的情况？
4. 在生产中遇到类似问题应如何解决？]


```

**步骤5：调度故障排查（标签匹配失败场景）**
**🎯 学习目的**：理解 nodeSelector 等调度约束的作用，掌握调度故障排查的常用方法
**🔗 整体关系**：从资源约束转向节点关系约束，完整了解调度器的筛选机制

```bash
# 5.1 创建一个无法调度的 Pod（使用不存在的节点标签）
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: unschedulable-pod
spec:
  nodeSelector:
    nonexistent-label: "true"
  containers:
  - name: nginx
    image: nginx:latest
EOF
```

**💻 命令输出记录：**
```
[记录使用nodeSelector的Pod创建结果]


```

```bash
# 5.2 分析调度失败原因 - 查看Pod详情
kubectl describe pod unschedulable-pod
```

**💻 命令输出记录：**
```
[记录nodeSelector相关的调度失败信息：
- NodeSelector配置
- Pod的Pending状态
- 调度失败的具体原因
- 节点选择器不匹配的错误信息]


```

```bash
# 5.3 查看最近的调度相关事件
kubectl get events --sort-by='.lastTimestamp' | tail -10
```

**💻 命令输出记录：**
```
[记录最近的集群事件：
- unschedulable-pod的调度失败事件
- 具体的错误原因
- 时间戳和重试情况]


```

```bash
# 5.4 验证调度失败原因 - 查看节点标签
kubectl get nodes --show-labels
```

**💻 命令输出记录：**
```
[验证调度失败原因：
- 查看所有节点的标签
- 确认没有节点包含nonexistent-label=true标签
- 理解nodeSelector的匹配原理]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. nodeSelector如何影响调度决策？
2. 遇到调度失败时的排查步骤是什么？
3. 如何区分不同类型的调度失败原因？
4. 生产中常见的调度约束有哪些？]


```

**清理操作**：
```bash
# 清理所有测试Pod
kubectl delete pod simple-pod resource-pod large-resource-pod unschedulable-pod
```

**💻 命令输出记录：**
```
[记录清理结果，确认所有测试Pod都被成功删除]


```

**📝 Day 4-5 整体学习总结**：
```
[对Scheduler调度机制的深度理解总结：

1. 调度器工作原理：
   - 预选（Filtering）阶段：筛选可用节点
   - 优选（Scoring）阶段：评分选出最佳节点
   - 绑定（Binding）阶段：将Pod分配给节点

2. 调度约束类型：
   - 资源约束：CPU/内存requests和limits
   - 节点选择约束：nodeSelector、nodeAffinity
   - Pod亲和性约束：podAffinity/podAntiAffinity

3. 调度失败处理：
   - 调度失败时Pod保持Pending状态
   - 通过Events和Describe命令诊断问题
   - 调度器会持续重试直到成功

4. 生产应用最佳实践：
   - 合理设置资源requests和limits
   - 使用节点亲和性优化调度
   - 监控调度器性能和健康状态]


```

**🔍 知识点检查清单**：
- ☐ 能够描述 Pod 调度的完整流程（从创建到绑定）
- ☐ 理解调度器如何处理资源约束和节点选择
- ☐ 掌握预选（Filtering）和优选（Scoring）的区别和作用
- ☐ 能够诊断和解决常见的调度失败问题
- ☐ 理解 nodeSelector、nodeAffinity 等调度约束的使用

#### 📅 Day 6-7: 深度阅读和整体串联（1-2小时）
**学习优先级**：⭐ 综合提升 | **前置要求**：完成控制器和调度器学习
**依赖关系**：需要前面所有组件知识，进行整体流程串联
**学习建议**：🎯 这是第2周的总结，务必完成以检验学习效果
**学习内容**：
- [ ] 阅读：[What happens when I type kubectl run?](https://github.com/jamiehannaford/what-happens-when-k8s)
- [ ] 中文版：[kubectl 创建 Pod 背后到底发生了什么？](https://mp.weixin.qq.com/s/ctdvbasKE-vpLRxDJjwVMw)

**详细实践步骤**：

**步骤1：综合流程追踪实验**
**🎯 学习目的**：综合运用第2周所学知识，追踪和理解 Pod 创建的完整流程
**🔗 整体关系**：作为第2周学习的总结和验证，为第3周Node组件学习做准备

```bash
# 1.1 在多个终端窗口同时监控组件日志（分别在不同终端运行）
# 终端 1: 监控事件
kubectl get events --watch
```

**💻 命令输出记录（持续记录）：**
```
[实时记录所有集群事件，重点关注接下来的Pod创建过程]


```

```bash
# 终端 2: 监控调度器日志
kubectl logs -n kube-system -l component=kube-scheduler -f
```

**💻 命令输出记录（持续记录）：**
```
[实时记录调度器的决策过程]


```

```bash
# 终端 3: 监控控制器日志
kubectl logs -n kube-system -l component=kube-controller-manager -f
```

**💻 命令输出记录（持续记录）：**
```
[实时记录控制器的工作日志]


```

```bash
# 1.2 在主终端创建 Pod 并观察整个流程
kubectl run trace-pod --image=nginx:latest
```

**💻 命令输出记录：**
```
[记录Pod创建命令的结果，同时关注其他终端的实时日志输出]


```

```bash
# 1.3 分析Pod的详细信息和事件时序
kubectl describe pod trace-pod
```

**💻 命令输出记录：**
```
[记录Pod的完整创建过程：
- 创建时间和调度信息
- 事件时间线和组件交互
- 最终的运行状态]


```

```bash
# 1.4 查看按时间排序的相关事件
kubectl get events --sort-by='.lastTimestamp' | grep trace-pod
```

**💻 命令输出记录：**
```
[按时间顺序记录trace-pod的所有事件：
- 从创建到调度的时间间隔
- 从调度到启动的时间间隔
- 各个组件参与的时间点]


```

```bash
# 1.5 清理测试资源
kubectl delete pod trace-pod
```

**💻 命令输出记录：**
```
[记录Pod删除过程，观察事件和组件日志的变化]


```

**📝 本步骤学习收获总结：**
```
[总结第2周整体学习成果：
1. 能够追踪和理解Pod创建的完整流程吗？
2. 各个组件（API Server、Controller Manager、Scheduler）的协作关系清楚吗？
3. 从细节中能看出哪些性能瓶颈或优化点？
4. 为第3周学习Node组件做好准备了吗？]


```

---

### 第3周：Node 组件和网络（8-10小时）
**学习主线**：从 Control Plane 转向 Node，先 Kubelet → 容器运行时 → 网络组件

#### 📅 Day 1-3: Kubelet 深入理解（4-5小时）
**学习优先级**：🔥 核心必修 | **前置要求**：完成第2周调度器学习
**依赖关系**：理解调度器分配 Pod 后，学习 Kubelet 如何在节点上运行 Pod
**学习建议**：✅ 这是理解 Pod 实际运行的关键组件
**学习内容**：
- [ ] Kubelet 的职责和工作机制
- [ ] Pod 生命周期管理
- [ ] 容器运行时接口（CRI）

**详细实践步骤**：

**步骤1：Kubelet 状态分析**
**🎯 学习目的**：了解 Kubelet 在节点上的运行状态、配置和日志，为深入理解Pod生命周期管理打基础
**🔗 整体关系**：从 Control Plane 组件转向 Node 组件，理解调度器分配 Pod 后的实际执行过程

```bash
# 1.1 进入节点查看 Kubelet 进程
minikube ssh
```

**💻 命令输出记录：**
```
[记录进入minikube节点的结果，注意提示符变化]


```

```bash
# 1.2 在minikube节点内查看Kubelet进程
ps aux | grep kubelet
```

**💻 命令输出记录：**
```
[记录Kubelet进程信息：
- 进程ID和父进程
- 启动参数和配置文件路径
- CPU和内存使用情况
- 运行用户和权限]


```

```bash
# 1.3 查看 Kubelet 配置文件
sudo cat /var/lib/kubelet/config.yaml
```

**💻 命令输出记录：**
```
[记录Kubelet的重要配置参数：
- clusterDNS和clusterDomain设置
- 容器运行时配置（containerd/docker）
- 资源管理和限制设置
- 认证和授权配置]


```

```bash
# 1.4 查看 Kubelet 服务日志
sudo journalctl -u kubelet --no-pager --lines=50
```

**💻 命令输出记录：**
```
[记录Kubelet日志中的关键信息：
- Pod创建和销毁日志
- 容器运行时交互
- 资源监控和上报
- 任何错误或警告信息]


```

```bash
# 1.5 退出minikube节点
exit
```

**💻 命令输出记录：**
```
[记录退出结果，回到本地终端]


```

**📝 本步骤学习收获总结：**
```
[总结：
1. Kubelet在节点上的运行状态如何？
2. Kubelet的配置中哪些参数最关键？
3. 从日志中能看出Kubelet的哪些工作行为？
4. Kubelet如何与容器运行时交互？]


```

**步骤2：Pod 生命周期实验**
```bash
# 2.1 创建带有生命周期钩子的 Pod
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo 'PostStart executed' > /tmp/lifecycle.log"]
      preStop:
        exec:
          command: ["/bin/sh", "-c", "echo 'PreStop executed' >> /tmp/lifecycle.log; sleep 10"]
EOF

# 2.2 监控 Pod 启动过程
kubectl get pod lifecycle-pod --watch

# 2.3 查看生命周期事件
kubectl describe pod lifecycle-pod
kubectl exec lifecycle-pod -- cat /tmp/lifecycle.log

# 2.4 删除 Pod 观察停止过程
kubectl delete pod lifecycle-pod --grace-period=30
```

**步骤3：容器运行时探索**
```bash
# 3.1 查看节点上的容器
minikube ssh
sudo docker ps | head -10

# 3.2 查看具体容器信息
CONTAINER_ID=$(sudo docker ps | grep nginx | awk '{print $1}' | head -1)
sudo docker inspect $CONTAINER_ID | head -20
```

**知识点检查**：
- Kubelet 如何与 API Server 通信？
- Pod 在节点上的完整生命周期？
- 容器运行时接口的作用是什么？

#### 📅 Day 4-5: Container Runtime 探索（2-3小时）
**学习优先级**：🟡 深入理解 | **前置要求**：完成 Kubelet 学习
**依赖关系**：Kubelet 通过 CRI 与容器运行时交互，先学 Kubelet 再学运行时
**学习建议**：💡 偏底层，如果时间紧张可以简化，重点理解 CRI 接口
**学习内容**：
- [ ] Docker、Containerd、CRI-O 对比
- [ ] 容器镜像管理
- [ ] 容器网络基础

**实践任务**：
- [ ] 使用 `docker ps` 或 `crictl ps` 查看容器
- [ ] 理解镜像拉取过程
- [ ] 观察容器日志和资源使用

#### 📅 Day 6-7: Kube-proxy 和网络（2-3小时）
**学习优先级**：🔥 核心必修 | **前置要求**：完成前面 Node 组件学习
**依赖关系**：理解 Pod 如何运行后，学习 Pod 间如何通信
**学习建议**：✅ 网络是集群的血脉，必须理解基本原理
**学习内容**：
- [ ] Service 的实现原理
- [ ] iptables 和 IPVS 模式
- [ ] 网络插件（CNI）基础

**实践任务**：
- [ ] 创建 Service，查看 iptables 规则
- [ ] 理解 ClusterIP、NodePort、LoadBalancer

---

### 第4周：核心插件和实战（8-10小时）
**学习主线**：从基础设施到综合实践，先 DNS → 网络插件 → 手动部署实战

#### 📅 Day 1-2: DNS 服务发现（3-4小时）
**学习优先级**：🔥 核心必修 | **前置要求**：完成第3周网络基础
**依赖关系**：理解网络通信后，学习服务发现机制
**学习建议**：✅ DNS 是微服务架构的基础，务必掌握
**学习内容**：
- [ ] CoreDNS 配置和工作原理
- [ ] Kubernetes 中的 DNS 解析
- [ ] Service Discovery 机制

**实践任务**：
- [ ] 测试 Pod 间的 DNS 解析
- [ ] 查看 CoreDNS 配置
- [ ] 创建自定义 DNS 记录

#### 📅 Day 3-4: 网络插件理解（3-4小时）
**学习优先级**：🟡 深入理解 | **前置要求**：完成 DNS 学习
**依赖关系**：DNS 依赖于网络插件提供的网络连通性
**学习建议**：💡 网络插件偏底层，重点理解 CNI 概念和 Flannel 基本原理
**学习内容**：
- [ ] CNI 插件架构
- [ ] Flannel 工作原理
- [ ] Pod 网络通信机制

**实践任务**：
- [ ] 查看网络插件配置
- [ ] 测试跨节点 Pod 通信
- [ ] 理解网络策略基础

#### 📅 Day 5-7: 综合实践 - 手动部署集群（2-3小时）
**学习优先级**：⭐ 综合应用 | **前置要求**：完成前面所有核心组件学习
**依赖关系**：需要前面所有知识，进行综合实践验证
**学习建议**：🎯 这是第1-4周学习的综合检验，强烈建议完成
**实践任务**：
- [ ] 完成 [lab3-manual-installation](../tutorials/lab3-manual-installtion.md)
- [ ] 尝试手动安装 Kubernetes 组件
- [ ] 排查和解决安装过程中的问题

---

### 第5-6周：深化理解和高级实战（8-16小时）
**学习主线**：从理论到实战，先组件交互分析 → 故障排查 → 性能优化

#### 🎯 学习路径说明
**前置要求**：✅ 必须完成前4周的核心学习
**学习建议**：这是高级阶段，建议根据个人时间和兴趣选择重点项目

#### 📅 Week 5 Day 1-3: 组件交互深度分析（4-6小时）
**学习优先级**：⭐ 综合提升 | **前置要求**：完成所有核心组件学习
**依赖关系**：需要对所有组件有深入理解，才能分析组件间交互
**实践目标**：
- [ ] 追踪一个 Pod 从创建到运行的完整流程
- [ ] 分析各组件的日志和事件
- [ ] 绘制详细的交互时序图

#### 📅 Week 5 Day 4-7: 故障排查技能训练（4-6小时）
**学习优先级**：🔥 实用技能 | **前置要求**：完成组件交互分析
**依赖关系**：基于对正常流程的理解，学习异常情况的处理

**实践目标**：
- [ ] 模拟组件故障
- [ ] 练习日志分析和问题定位
- [ ] 学习常见问题的解决方法

#### 📅 Week 6 Day 1-3: 性能监控和优化（2-4小时）
**学习优先级**：🟡 进阶提升 | **前置要求**：完成故障排查学习
**依赖关系**：在理解正常和异常情况后，学习性能优化

**实践目标**：
- [ ] 使用 `kubectl top` 监控资源使用
- [ ] 理解资源配额和限制
- [ ] 学习集群性能调优基础

## 📚 推荐学习资源

### 必读文档
- [ ] [Kubernetes 官方架构文档](https://kubernetes.io/docs/concepts/architecture/)
- [ ] [Kubernetes 指南 - feisky](https://kubernetes.feisky.xyz/concepts/index)
- [ ] [Etcd 官方文档](https://etcd.io/docs)

### 实践工具
- [ ] `kubectl` 命令行工具
- [ ] `minikube` 或 `kind` 本地环境
- [ ] 日志查看和监控工具

### 扩展阅读
- [ ] 《Kubernetes in Action》相关章节
- [ ] 官方 Blog 中的架构文章
- [ ] CNCF 相关技术分享

## ✅ 阶段检查清单

### 理论理解检查
- [ ] 能画出 Kubernetes 完整架构图
- [ ] 能解释 Pod 创建的完整流程
- [ ] 理解控制器模式和声明式 API
- [ ] 掌握调度器的工作原理
- [ ] 了解网络和存储的基本原理

### 实践技能检查
- [ ] 能够查看和分析各组件日志
- [ ] 能够使用 kubectl 进行集群诊断
- [ ] 能够解释集群中发生的事件
- [ ] 具备基本的故障排查能力

### 核心问题回答
在完成本阶段学习后，你应该能够回答：
1. **Kubernetes 组件是如何交互，来启动容器，并对外提供服务的？**
2. **当我执行 `kubectl apply -f deployment.yaml` 时，集群内部发生了什么？**
3. **Pod 调度和启动的完整流程是什么？**
4. **如何诊断和解决常见的集群问题？**

## 💡 学习建议

### 学习方法
1. **理论与实践结合**：每学完一个概念，立即通过实验验证
2. **绘图记忆**：通过画图加深对架构的理解
3. **日志分析**：养成查看日志的习惯
4. **问题驱动**：带着问题去学习，寻找答案

### 时间分配建议
- **理论学习**: 40%（阅读文档、博客）
- **实践操作**: 45%（命令行实验、环境搭建）
- **总结整理**: 15%（笔记、思维导图）

### 遇到困难时
1. 查看官方文档和日志
2. 搜索相关问题和解决方案
3. 在社区论坛提问
4. 与其他学习者交流讨论

记住：**筑基期是比较困难的阶段，请不要气馁！理解架构原理是后续学习的重要基础。** 💪

---

## 📊 学习进度跟踪表

### 第1周完成情况
- [ ] **Day 1-2**: Control Plane & Node 架构理解
  - [ ] 环境准备和集群信息获取
  - [ ] 系统组件探索  
  - [ ] 组件配置和通信分析
  - [ ] 架构图绘制
  
- [ ] **Day 3-4**: API Server 深入理解
  - [ ] API Server 代理访问
  - [ ] 直接 API 调用实验
  - [ ] API 操作实验
  - [ ] API Server 日志分析
  - [ ] API 版本探索
  
- [ ] **Day 5-7**: Etcd 存储机制
  - [ ] Etcd 状态探索
  - [ ] 通过 API Server 查看数据
  - [ ] 数据变化观察实验
  - [ ] 备份恢复理解

### 第2周完成情况
- [ ] **Day 1-3**: Controller Manager 核心理念
  - [ ] 控制器运行状态分析
  - [ ] Deployment 控制器实验
  - [ ] 控制器自愈能力测试
  - [ ] 扩缩容行为观察
  
- [ ] **Day 4-5**: Scheduler 调度机制
  - [ ] 调度器状态分析
  - [ ] 基本调度实验
  - [ ] 资源请求调度实验
  - [ ] 调度约束实验
  - [ ] 调度故障排查
  
- [ ] **Day 6-7**: 深度阅读和理解
  - [ ] 完整流程追踪实验
  - [ ] 阅读推荐文章

### 第3-6周完成情况
- [ ] **第3周**: Node 组件和网络
  - [ ] Kubelet 深入理解
  - [ ] Container Runtime 探索
  - [ ] Kube-proxy 和网络

- [ ] **第4周**: 核心插件和实战
  - [ ] DNS 服务发现
  - [ ] 网络插件理解
  - [ ] 综合实践

- [ ] **第5-6周**: 深化理解和实战
  - [ ] 组件交互分析
  - [ ] 故障排查练习
  - [ ] 性能监控优化

## 🎯 每日学习建议

### 实践技巧
- **多终端并行**：同时开启多个终端窗口监控不同组件
- **日志分析**：养成查看日志的习惯，日志是最好的老师
- **截图记录**：重要的实验结果建议截图保存
- **笔记整理**：建议使用 Markdown 格式记录学习笔记

### 时间分配参考
- **理论阅读**：30%
- **动手实践**：50%
- **笔记整理**：10%
- **问题解决**：10%